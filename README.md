# ML2_Final_Project
This project can be viewed as a continued project of the DTSA 5509 final project "Predicting Sleep Quality with Supervised Machine Learning Methods: A Case Study." It predicted whether the subject slept well or not, i.e., simplified the multi-level sleep quality level prediction problem to a binary class classification problem, using purely supervised machine learning approaches - logistic regression (LR), decision tree (DT), adaboost (AdaB), random forest (RF), and support vector machine (SVM) models. It was concluded that the SVM model performed the best, with an AUC of 0.72, an accuracy of 0.62, and an F1-score of 0.62. In this project, I want to cope with the multi-level sleep quality level prediction problem as a multi-class classification problem, as well as compare the purely supervised SVM model with hybrids of unsupervised clustering approaches and a supervised SVM classifier. In addition, I want to improve the data cleaning process by doing feature scaling and categorical variable encoding if applicable. I will also look into the possibility of overfitting in more detail.

Here are the models I will build in this project:
1. the SVM model: a purely supervised model
2. the KMeans_SVM model: a hybrid model of an unsupervised k-means clustering and a supervised SVM classifier, with hyperparameters to be tuned together
3. the Agg_SVM model: a hybrid model of an unsupervised agglomerative hierarchical clustering and a supervised SVM classifier, with hyperparameters to be tuned together

Unfortunately, since the dataset is small and imbalanced (when treated as a multi-class classification problem), all the models overfit. I tried to fix this issue by applying feature scaling to X_train and X_test to tackle the models' sensitivity on distance metrics, and running 5-fold cross validations to find the best suited hyperparameters. In addition, I  manually set the hyperparameter 'class_weight' in SVM as'balanced' to balance the classes. However, the models still overfit. I also tried but failed to split the data using stratificated sampling, because y_test does not contain all the class labels. More advanced resampling technique like SMOTE also turned out to be not applicable here, since a few minority classes has only 1 instance each. Random oversampling and other resampling techniques will introduce higher risk of overfitting. Thus, gathering more real-world samples is likely the only solution to this issue. <br>

If I were to conduct this multi-class classification project again, I will ask the subject to record at least 3 months of his sleep instances in order to get a larger, more balanced dataset. I believe this will fix the issue of model overfitting, and will result in models with good performances. 

Nevertheless, your suggestions for techniques to mitigate the impact of such an imbalanced, small dataset are very welcomed! Even though my project is flawed, still, please do not use my code or text without proper citation.

Thank you so much folks! I got a very good feedback on this project. I have to admit that I made a very silly, classic machine learning mistake - I got training accuracy scores using the same training dataset that I trained my models with; that is, I did not have a different validation dataset. Naturally the training accuracy scores are close to 1; while, the test accuracy scores are much lower, indicating a sever overfitting issue. If I were to redo this project with the original small dataset, I would use the Leave-One-Out Cross-Validation (LOOCV) technique to divide my data and evaluate my models, as suggested by a peer. If I collected a much larger dataset, I would use the standard Cross-Validation techniqe instead of simply splitting my data into a training, validation, and test set, because cross-validation "removes the 'chance' factor of getting a good/bad test split which scores high/low," quoting the original feedback.
